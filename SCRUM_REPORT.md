# Doc2Data - SCRUM Report

## Sprint Update - January 7, 2026 (End-to-End Status)

### Current Status (High-level)

- **Deployment**: DGX services are up (FastAPI + Streamlit) and stable with Docker restart policy.
- **Pipeline behavior**: CMS-1500 is no longer ‚Äúhardcoded to one sample‚Äù ‚Äî it now **chooses an extraction strategy** based on whether the PDF is truly digital vs scanned.
- **Main blocker**: **Handwritten / scanned CMS-1500 accuracy is still not acceptable** for critical fields (patient name, insured ID, etc.). We are extracting *something*, but it often contains **printed labels + nearby field bleed**, not clean values.

### Live URLs (Tailscale)

| Service | URL | Description |
|---------|-----|-------------|
| **FastAPI** | http://100.126.216.92:8000 | REST API endpoints |
| **API Docs** | http://100.126.216.92:8000/docs | Swagger UI |
| **Streamlit** | http://100.126.216.92:8501 | Web UI |

### What We Achieved So Far (Real, verified changes)

#### 1) CMS-1500 ‚ÄúRahul vs Rohit‚Äù bug: fixed at the source
- **Root cause**: some PDFs contain a **hidden/template digital text layer** that does not match visible pixels.
- **Fix**: added **visual validation** for the digital layer and only use it if it looks like real filled values (anchor-field QA check).
- **Result**: scanned CMS-1500 documents now **force visual OCR** (no more reading hidden text as truth).

#### 2) Template alignment: stabilized (no ‚Äútilted straight templates‚Äù)
- **Root cause**: unstable alignment refinements can introduce tilt/warp on already-straight templates.
- **Fix**: replaced the fragile alignment path with a **safer perspective/corner-based alignment** + stricter homography validation (reject warp/perspective drift).
- **Additional fix**: for CMS-1500 scans, we now **skip deskew before alignment** (deskew on handwriting can rotate the page incorrectly; alignment should handle rotation instead).

#### 3) CMS-1500 scanned extraction: removed ‚Äúsingle-sample hardcoding‚Äù
- **Old failure mode**: applying schema boxes in fixed pixel locations even when alignment is wrong ‚Üí totally incorrect crops.
- **Current behavior**:
  - If **digital layer is validated** ‚Üí match zones using digital words (best case).
  - Else (scan):
    - Attempt alignment; if alignment quality is high ‚Üí **full-page OCR once**, then **zone-match words** into schema fields.
    - If alignment is not reliable ‚Üí fallback to **full-page OCR + line grouping** (no fake schema boxes).

#### 4) General forms ‚Äúgiant box‚Äù failure mode: mitigated
- **Root cause**: layout detection sometimes returns 0‚Äì1 blocks or one near-full-page FIGURE block.
- **Fix**: drop giant blocks and **fallback to OCR line grouping** when layout is too coarse (<3 blocks).

### What‚Äôs Still Broken (Current Issues)

#### A) CMS-1500 handwritten/scanned field values are noisy / incorrect
Even with alignment success, some fields contain:
- **Pre-printed labels** (e.g., ‚ÄúPATIENT‚ÄôS NAME ‚Ä¶‚Äù) mixed into the value.
- **Zone bleed / cross-field leakage** (neighboring text is pulled into the zone).
- **Handwriting OCR errors** (PaddleOCR misses strokes; TrOCR can hallucinate on weak crops).

Example (from latest DGX run on `cms1500_handwritten.pdf`):
- Alignment quality ~0.98 (good).
- Extracted `2_patient_name` still contains label text + nearby address text.
- Extracted `1a_insured_id` contains label text + name bleed.

#### B) ‚ÄúIt runs in a loop / no results‚Äù symptom (why it looks stuck)
This is not an infinite loop; it‚Äôs repeated work:
- **Many OCR calls**: without careful gating, the pipeline can OCR **per-field crops** (48 CMS-1500 zones), which is slow and spams logs.
- **Model init spam**: Paddle/PaddleX prints ‚Äúmodel files already exist / using cached files‚Äù repeatedly; TrOCR loads transformers and logs warnings.
- **Connectivity check**: PaddleX prints ‚ÄúChecking connectivity to the model hosters‚Ä¶‚Äù which can add delay/noise even if models are cached.

**What we changed to reduce this:**
- Prefer **full-page OCR once + zone matching** for CMS-1500 scans (reduces N√ó OCR calls).
- Added guards to avoid re-OCR when a block already has text from zone matching (still needs tightening for schema-zone blocks and for empty-field handling).

### Root Causes (Technical)

1. **Deskew before alignment on handwriting** can introduce a small rotation error ‚Üí causes alignment/matching instability.
2. **Per-field crop OCR** is fragile:
   - tiny crops lose context;
   - printed labels dominate;
   - overlapping zones create duplicates/bleed;
   - OCR engines behave poorly on low-ink/noisy crops.
3. **Digital text layer can be wrong** (template-only/hidden text), so using it blindly creates wrong names/IDs.
4. **Zone matching needs stronger de-bleed**:
   - stricter word assignment / overlap rules,
   - better suppression of printed form text (template-diff at full-page scale),
   - field-specific cleanup rules.

### Next Steps (Required for ‚Äúproper results‚Äù)

#### CMS-1500 (priority)
- **Value isolation**: use **template-diff / printed-text suppression** at scale (not just per-crop) so the OCR words fed into zone matching are mostly ‚Äúink‚Äù (filled values), not labels.
- **De-bleed**: strengthen the zone matching assignment (unique assignment + intersection-over-area thresholds) and reduce zone padding for problematic fields.
- **Field-specific parsing**: parse name/id/date fields with stricter regex/validation and reject garbage.
- **Handwriting strategy**: TrOCR should be used only when there is strong ink and Paddle is clearly failing; otherwise it hallucinates.

#### End-to-end verification (still pending)
- Re-verify via Streamlit/API for:
  - **CMS1500 scan** (handwritten),
  - **UB-04 sample**,
  - **TCCC sample**.

### Repro / Debug Commands (DGX)

```bash
# Connect to DGX
ssh -i ../../dgx-spark/tailscale_spark2 radiant-dgx2@100.126.216.92

# Logs
docker logs -f doc2data-server

# Quick pipeline test inside container
docker exec doc2data-server python3 -c "from src.pipelines.multi_agent_pipeline import MultiAgentPipeline,PipelineConfig; p=MultiAgentPipeline(PipelineConfig(enable_slm_labeling=False,enable_vlm_figures=False,enable_alignment=True)); r=p.process_sync('/app/data/sample_docs/cms1500_handwritten.pdf'); print(r.get('form_type'), r.get('alignment_quality')); ef=r.get('extracted_fields') or {}; print('2_patient_name', ef.get('2_patient_name')); print('1a_insured_id', ef.get('1a_insured_id'))"
```

---

## Sprint Update - December 27, 2025

### ‚úÖ DEPLOYMENT COMPLETE - SERVER IS LIVE!

#### üöÄ Access URLs (For Team Members on Tailscale Network)

| Service | URL | Description |
|---------|-----|-------------|
| **FastAPI** | http://100.126.216.92:8000 | REST API endpoints |
| **API Docs** | http://100.126.216.92:8000/docs | Swagger UI - Interactive API documentation |
| **Streamlit** | http://100.126.216.92:8501 | Web UI for document processing |

#### üì° API Endpoints

1. **POST `/extract/reducto`** - Extract and get Reducto-compatible JSON output
2. **POST `/extract/v2`** - Full extraction with all metadata
3. **POST `/extract/cms1500`** - CMS-1500 specific extraction
4. **POST `/extract/generic`** - Generic document extraction

#### üîó How to Share with Team

**Prerequisites:** Team members must be on the **Tailscale network (radiantt.com)**

1. Install Tailscale: https://tailscale.com/download
2. Login with: rahul370139@gmail.com (or team admin)
3. Access: http://100.126.216.92:8000/docs

---

### ‚úÖ COMPLETED TASKS

#### 1. Detectron2 Configuration Fixed
- **Issue:** Was using wrong config `ppyolov2_r50vd_dcn_365e` (PaddleDetection) instead of `faster_rcnn_R_50_FPN_3x` (Detectron2)
- **Fix:** Updated LayoutDetectionAgent to use correct Detectron2 config with PaddleDetection fallback

#### 2. OCR Pipeline Verified
- **Test Result:** 47 fields extracted from CMS-1500 sample
- **Reducto Format:** Working with 48 blocks
- **Digital Text Layer:** Validated (score 3/3)

#### 3. Preprocessing Improvements
- Red-line removal for CMS-1500 forms
- Deskewing up to 15 degrees for scanned documents
- Template alignment with SIFT/ORB + ECC refinement

#### 4. Persistent Hosting on DGX
- Docker container running with `--restart unless-stopped`
- FastAPI starts immediately (no blocking on model downloads)
- Ollama models download in background

#### 5. Multi-Agent Pipeline Architecture
- Form identification (CMS-1500, UB-04, TCCC, Generic)
- Template alignment for known forms
- Tiered OCR (PaddleOCR + TrOCR fallback)
- Schema-zone matching for accurate field extraction

---

### üìä Test Results

```
‚úÖ Form Type: cms-1500
‚úÖ Fields Extracted: 47
‚úÖ Reducto format present
   - Chunks: 1
   - Blocks: 48
‚úÖ Digital text layer validated (score 3/3)
```

---

### üîß Useful Commands (SSH to DGX)

```bash
# Connect to DGX
ssh -i ../../dgx-spark/tailscale_spark2 radiant-dgx2@100.126.216.92

# View logs
docker logs -f doc2data-server

# Restart server
docker restart doc2data-server

# Stop server
docker stop doc2data-server
```

---

### üìÅ Project Structure

```
doc2data/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api_main.py         # FastAPI endpoints
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_main.py   # Streamlit UI
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_agent_pipeline.py  # Core extraction logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ business_schema.py       # Schema mapping
‚îÇ   ‚îú‚îÄ‚îÄ processing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py         # Image enhancement
‚îÇ   ‚îî‚îÄ‚îÄ ocr/
‚îÇ       ‚îî‚îÄ‚îÄ paddle_ocr.py            # PaddleOCR wrapper
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ schemas/cms-1500.json        # Field definitions
‚îÇ   ‚îî‚îÄ‚îÄ sample_docs/                 # Test documents
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ start_services.sh       # Service startup script
‚îú‚îÄ‚îÄ deploy_and_run.sh       # Deployment automation
‚îî‚îÄ‚îÄ requirements_docker.txt
```

---

### üêç Python API Client Example

```python
import requests

# Upload a PDF and get Reducto-style output
url = "http://100.126.216.92:8000/extract/reducto"
with open("your_cms1500.pdf", "rb") as f:
    response = requests.post(url, files={"file": f})
    
result = response.json()
print(result["result"]["chunks"][0]["content"])
```

---

### ‚ö†Ô∏è Known Limitations

1. **Handwritten Forms:** OCR accuracy may vary for heavily handwritten content
2. **VLM/SLM:** Requires Ollama model download (~2GB) - runs in background
3. **GPU:** Optimized for NVIDIA DGX with GPU acceleration

---

### üìù Next Steps (Optional Improvements)

1. [ ] Add authentication to API
2. [ ] Set up public HTTPS access with cloudflared tunnel
3. [ ] Add batch processing endpoint
4. [ ] Fine-tune YOLO model for better field detection
